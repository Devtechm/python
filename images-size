from urllib.request import Request, urlopen
import re
from bs4 import BeautifulSoup
import pandas as pd
import requests
import shutil
import urllib.request
import time
from urllib.parse import urlparse


count = 1
names = []
imgTitle = []
links = []
path = '/users/don/desktop/'
infile = path + "web1.txt"
f = open(infile, "w")
url = 'https://web.com'

while (count < 7):
    site = 'https://www.web.com/search/town-food.html?page=' + str(count) 
    req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})
    html_page = urlopen(req).read()
    soup = BeautifulSoup(html_page, "lxml")
    
    for link in soup.find_all("img"):
        img = link.get('src')
        title = link.get('alt')
        if img.startswith("/images/products/medium/"):
            f.write(url + img)
            f.write("\n")
            links.append(img)
            names.append(title)

    count+=1

f.close()

for name in names:
    count=1
    if name.startswith("Town "):
        if name.startswith("Town Bamboo"):
            name.strip()
        elif name.startswith("Town 5 "):
            name = name.replace(' - 12/Pack', '')
            name = name.split(" ")[-1].strip()

        else:
            name = name.replace('Town', '')
            name = name.split(" ")[1].strip()
        
    name.replace('/', '_')
    if name in imgTitle:
        name = name + "_" + str(count)
        count+=1
        # print(name)
            
    
    imgTitle.append(name)

count=0
outfile = path + "web2.txt"
with open(infile) as fin, open(outfile, 'w') as fout:
    for link in fin:
        if link.startswith("https://"):
            format = link[-5:]
            link = link.replace('small', 'extra_large').replace('medium', 'extra_large')
            
	    if is_url(link) != true:
		link = link.replace('extra_large', 'large')

            #response = requests.get(link)
            #if response.status_code != 200:
                #link = link.replace('extra_large', 'large')
                #print(link)

            fileName = (imgTitle[count] + format).strip().lower()

            # opener=urllib.request.build_opener()
            # opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]
            # urllib.request.install_opener(opener)
            # urllib.request.urlretrieve(link, fileName)
            # try:
            #     link = link.replace('small', 'extra_large').replace('medium', 'extra_large')
            #     urllib.request.urlretrieve(link, fileName)

            # except:
            #     link = link.replace('extra_large', 'large')
            #     urllib.request.urlretrieve(link, fileName)

            fout.write(link)
            # print(fileName, " ", link)
            time.sleep(3)
        
        count+=1

print(count)


def is_url(url):
  try:
    result = urlparse(url)
    return all([result.scheme, result.netloc])
  except ValueError:
    return False
